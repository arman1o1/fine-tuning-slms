{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóûÔ∏è Fine-tuning RoBERTa on AG News Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "In this notebook, we'll fine-tune **FacebookAI/roberta-base** on the **fancyzhx/ag_news** dataset for news topic classification. By the end of this notebook, you'll understand:\n",
    "\n",
    "1. **RoBERTa vs BERT** - Understanding the key differences\n",
    "2. **AG News Dataset** - A 4-class news topic classification dataset\n",
    "3. **Full Dataset Training** - Training on the complete dataset (120,000 samples)\n",
    "4. **Multiclass Classification** - Classifying into World, Sports, Business, and Sci/Tech\n",
    "5. **Inference Pipeline** - Using the fine-tuned model for predictions\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ About RoBERTa\n",
    "\n",
    "**RoBERTa** (Robustly Optimized BERT Pretraining Approach) is an improved version of BERT developed by Facebook AI. Key improvements include:\n",
    "\n",
    "| Aspect | BERT | RoBERTa |\n",
    "|--------|------|----------|\n",
    "| Training Data | 16GB | 160GB |\n",
    "| Training Steps | 1M | 500K |\n",
    "| Next Sentence Prediction | ‚úÖ Used | ‚ùå Removed |\n",
    "| Dynamic Masking | ‚ùå Static | ‚úÖ Dynamic |\n",
    "| Batch Size | 256 | 8K |\n",
    "\n",
    "RoBERTa generally achieves better results on NLP benchmarks!\n",
    "\n",
    "---\n",
    "\n",
    "## üì∞ About AG News Dataset\n",
    "\n",
    "The AG News dataset is a collection of news articles for topic classification:\n",
    "\n",
    "- **4 Classes**: World (0), Sports (1), Business (2), Sci/Tech (3)\n",
    "- **Training Samples**: 120,000\n",
    "- **Test Samples**: 7,600\n",
    "- **Balanced**: 30,000 samples per class in training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Setup & Installation\n",
    "\n",
    "Let's start by installing and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers datasets torch accelerate evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using CUDA GPU: \n",
      "Device selected: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"üçé Using Apple Silicon MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU (training will be slower)\")\n",
    "\n",
    "print(f\"Device selected: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Loading the AG News Dataset\n",
    "\n",
    "We'll load the full AG News dataset from the `fancyzhx/ag_news` repository on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading AG News dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7123c007670b4f9894f891e2c3420d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8592e7e8e1b4ee294b32aa229fe634a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b2e4082094444cb0ee49c23ab14a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58696cc0aaa44d19b8ac45bcd7384bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780e0494204a498b851ce605874baab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä AG News Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the AG News dataset\n",
    "print(\"üì¶ Loading AG News dataset...\")\n",
    "ag_news_dataset = load_dataset(\"fancyzhx/ag_news\")\n",
    "\n",
    "print(\"\\nüìä AG News Dataset Structure:\")\n",
    "print(ag_news_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Sample News Articles:\n",
      "======================================================================\n",
      "\n",
      "üåç World:\n",
      "   Venezuelans Vote Early in Referendum on Chavez Rule (Reuters) Reuters - Venezuelans turned out early\\and in large numbers on Sunday to vote in a histo...\n",
      "\n",
      "‚öΩ Sports:\n",
      "   Phelps, Thorpe Advance in 200 Freestyle (AP) AP - Michael Phelps took care of qualifying for the Olympic 200-meter freestyle semifinals Sunday, and th...\n",
      "\n",
      "üíº Business:\n",
      "   Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again....\n",
      "\n",
      "üî¨ Sci/Tech:\n",
      "   'Madden,' 'ESPN' Football Score in Different Ways (Reuters) Reuters - Was absenteeism a little high\\on Tuesday among the guys at the office? EA Sports...\n"
     ]
    }
   ],
   "source": [
    "# Define the label names for AG News\n",
    "label_names = {\n",
    "    0: \"üåç World\",\n",
    "    1: \"‚öΩ Sports\",\n",
    "    2: \"üíº Business\",\n",
    "    3: \"üî¨ Sci/Tech\"\n",
    "}\n",
    "\n",
    "# Examine a sample from each class\n",
    "print(\"üì∞ Sample News Articles:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for label_id in range(4):\n",
    "    # Find a sample with this label\n",
    "    for sample in ag_news_dataset['train']:\n",
    "        if sample['label'] == label_id:\n",
    "            print(f\"\\n{label_names[label_id]}:\")\n",
    "            print(f\"   {sample['text'][:150]}...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Label Distribution in Training Set:\n",
      "==================================================\n",
      "   üåç World: 30,000 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   ‚öΩ Sports: 30,000 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   üíº Business: 30,000 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   üî¨ Sci/Tech: 30,000 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "   Total training samples: 120,000\n",
      "   Total test samples: 7,600\n"
     ]
    }
   ],
   "source": [
    "# Check the label distribution\n",
    "from collections import Counter\n",
    "\n",
    "train_labels = ag_news_dataset['train']['label']\n",
    "label_counts = Counter(train_labels)\n",
    "\n",
    "print(\"üìà Label Distribution in Training Set:\")\n",
    "print(\"=\" * 50)\n",
    "for label in sorted(label_counts.keys()):\n",
    "    count = label_counts[label]\n",
    "    bar = \"‚ñà\" * (count // 1000)\n",
    "    print(f\"   {label_names[label]}: {count:,} {bar}\")\n",
    "\n",
    "print(f\"\\n   Total training samples: {len(train_labels):,}\")\n",
    "print(f\"   Total test samples: {len(ag_news_dataset['test']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî§ Part 2: Tokenization\n",
    "\n",
    "Now we'll tokenize the dataset using RoBERTa's tokenizer. RoBERTa uses **Byte-Pair Encoding (BPE)** tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cdfedbe8b54238849d9c30e84ce7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf535c542ee74b3ca5a41ef2d84ba64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274ec36396da48a984064ae8080aca15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6010481c12f45d0a2e71713a6eedd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26c0372eadb40a7af73ca3485ec2492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded tokenizer for: FacebookAI/roberta-base\n",
      "   Vocabulary size: 50,265 tokens\n",
      "   Model max length: 512\n"
     ]
    }
   ],
   "source": [
    "# Define the model checkpoint\n",
    "MODEL_CHECKPOINT = \"FacebookAI/roberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "print(f\"‚úÖ Loaded tokenizer for: {MODEL_CHECKPOINT}\")\n",
    "print(f\"   Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n",
    "print(f\"   Model max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenization Example:\n",
      "   Original: Tech giant Apple unveils new iPhone with AI-powered features at annual conference.\n",
      "\n",
      "   Tokens: ['Tech', 'ƒ†giant', 'ƒ†Apple', 'ƒ†unve', 'ils', 'ƒ†new', 'ƒ†iPhone', 'ƒ†with', 'ƒ†AI', '-', 'powered', 'ƒ†features', 'ƒ†at', 'ƒ†annual', 'ƒ†conference', '.']\n",
      "\n",
      "   Token IDs: [0, 14396, 3065, 1257, 36685, 5290, 92, 2733, 19, 4687, 12, 10711, 1575, 23, 1013, 1019, 4, 2]\n",
      "\n",
      "   Number of tokens: 16\n"
     ]
    }
   ],
   "source": [
    "# Let's see tokenization in action\n",
    "sample_text = \"Tech giant Apple unveils new iPhone with AI-powered features at annual conference.\"\n",
    "\n",
    "# Tokenize the sample\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "\n",
    "print(\"üî§ Tokenization Example:\")\n",
    "print(f\"   Original: {sample_text}\")\n",
    "print(f\"\\n   Tokens: {tokens}\")\n",
    "print(f\"\\n   Token IDs: {token_ids}\")\n",
    "print(f\"\\n   Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Tokenizing dataset... (this may take a minute)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390ac36ac01e40afb23a3e2adc873398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a265c8088c82426f975d07148281e440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete!\n",
      "\n",
      "üìä Tokenized Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the text with truncation.\n",
    "    \n",
    "    - truncation=True: Cuts longer texts to max_length\n",
    "    - max_length=256: Maximum sequence length\n",
    "    \n",
    "    Note: We'll use DataCollatorWithPadding for dynamic padding\n",
    "          which is more efficient than padding='max_length'\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the entire dataset\n",
    "print(\"‚è≥ Tokenizing dataset... (this may take a minute)\")\n",
    "tokenized_dataset = ag_news_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text to save memory\n",
    ")\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "\n",
    "# View the new structure\n",
    "print(\"\\nüìä Tokenized Dataset Structure:\")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Tokenized Example:\n",
      "   Keys: dict_keys(['label', 'input_ids', 'attention_mask'])\n",
      "   Input IDs length: 39\n",
      "   Attention mask length: 39\n",
      "   Label: 2 (üíº Business)\n"
     ]
    }
   ],
   "source": [
    "# Examine a tokenized example\n",
    "example = tokenized_dataset['train'][0]\n",
    "\n",
    "print(\"üîç Tokenized Example:\")\n",
    "print(f\"   Keys: {example.keys()}\")\n",
    "print(f\"   Input IDs length: {len(example['input_ids'])}\")\n",
    "print(f\"   Attention mask length: {len(example['attention_mask'])}\")\n",
    "print(f\"   Label: {example['label']} ({label_names[example['label']]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 3: Setting Up the Model\n",
    "\n",
    "Now we load the pretrained RoBERTa model and configure it for our 4-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddfe24e30ad4cfbb475cd2927dba424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded and moved to cuda\n",
      "   Model type: RobertaForSequenceClassification\n",
      "   Number of parameters: 124,648,708\n",
      "   Trainable parameters: 124,648,708\n"
     ]
    }
   ],
   "source": [
    "# Define id2label and label2id mappings for better model card\n",
    "id2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "label2id = {\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3}\n",
    "\n",
    "# Load the model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=4,  # 4-class classification\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded and moved to {device}\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "print(f\"   Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìê Part 4: Setting Up Evaluation Metrics\n",
    "\n",
    "We'll use accuracy as our evaluation metric, but also compute F1 score for a more comprehensive view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1136cbc1a44d66955cf96f77eff5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation metrics configured!\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy and F1 score from predictions.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1']\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Part 5: Training Configuration\n",
    "\n",
    "Let's set up our training parameters for 3 epochs on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured!\n",
      "   Epochs: 3\n",
      "   Batch size: 16\n",
      "   Learning rate: 2e-05\n",
      "   Total training samples: 120,000\n",
      "   Steps per epoch: 7,500\n"
     ]
    }
   ],
   "source": [
    "# Create data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output settings\n",
    "    output_dir=\"./roberta_ag_news_model\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate=2e-5,              # Standard LR for fine-tuning transformers\n",
    "    num_train_epochs=3,              # 3 epochs \n",
    "    per_device_train_batch_size=16,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=32,   # Larger batch for evaluation (no gradients)\n",
    "    weight_decay=0.01,               # Regularization\n",
    "    warmup_ratio=0.1,                # 10% warmup steps\n",
    "    \n",
    "    # Evaluation strategy\n",
    "    eval_strategy=\"epoch\",           # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",           # Save checkpoint after each epoch\n",
    "    load_best_model_at_end=True,     # Load the best model when training ends\n",
    "    metric_for_best_model=\"accuracy\", # Use accuracy to select best model\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,               # Log every 500 steps\n",
    "    \n",
    "    # Performance optimizations\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    gradient_accumulation_steps=1,   # Adjust if batch size needs to be larger\n",
    "    \n",
    "    # Other settings\n",
    "    seed=42,                         # For reproducibility\n",
    "    report_to=\"none\",                # Disable wandb/tensorboard reporting\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured!\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Total training samples: {len(tokenized_dataset['train']):,}\")\n",
    "print(f\"   Steps per epoch: {len(tokenized_dataset['train']) // training_args.per_device_train_batch_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ Understanding Training Hyperparameters\n",
    "\n",
    "| Parameter | Description | Our Value |\n",
    "|-----------|-------------|------------|\n",
    "| `learning_rate` | How much to update weights each step | 2e-5 |\n",
    "| `num_train_epochs` | Complete passes through training data | 3 |\n",
    "| `batch_size` | Samples processed before updating weights | 16 |\n",
    "| `weight_decay` | Regularization to prevent overfitting | 0.01 |\n",
    "| `warmup_ratio` | Fraction of steps for learning rate warmup | 0.1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized!\n",
      "   Training samples: 120,000\n",
      "   Evaluation samples: 7,600\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n",
    "print(f\"   Training samples: {len(tokenized_dataset['train']):,}\")\n",
    "print(f\"   Evaluation samples: {len(tokenized_dataset['test']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 6: Training the Model\n",
    "\n",
    "Now we start the fine-tuning process! This will train on the full 120,000 samples for 3 epochs.\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: Training on the full dataset may take 30-60 minutes on a GPU, or several hours on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting fine-tuning RoBERTa on AG News dataset...\n",
      "   This will train for 3 epochs on 120,000 samples.\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22500/22500 09:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.185017</td>\n",
       "      <td>0.942632</td>\n",
       "      <td>0.942530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.177822</td>\n",
       "      <td>0.950132</td>\n",
       "      <td>0.950147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>0.202116</td>\n",
       "      <td>0.954079</td>\n",
       "      <td>0.954104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting fine-tuning RoBERTa on AG News dataset...\")\n",
    "print(\"   This will train for 3 epochs on 120,000 samples.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training Metrics:\n",
      "   Total steps: 22,500\n",
      "   Training loss: 0.1910\n",
      "   Training runtime: 571.41 seconds\n",
      "   Samples per second: 630.01\n"
     ]
    }
   ],
   "source": [
    "# Display training metrics\n",
    "print(\"üìä Training Metrics:\")\n",
    "print(f\"   Total steps: {train_result.global_step:,}\")\n",
    "print(f\"   Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Training runtime: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating the model on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Final Evaluation Results:\n",
      "==================================================\n",
      "   Loss: 0.2021\n",
      "   Accuracy: 0.9541 (95.41%)\n",
      "   F1 Score: 0.9541\n",
      "   Runtime: 2.21 seconds\n",
      "   Samples/second: 3433.05\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "print(\"üìä Evaluating the model on test set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìà Final Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"   Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"   Accuracy: {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"   F1 Score: {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"   Runtime: {eval_results['eval_runtime']:.2f} seconds\")\n",
    "print(f\"   Samples/second: {eval_results['eval_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Part 7: Saving the Fine-tuned Model\n",
    "\n",
    "Let's save our model so we can use it later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: ./roberta_ag_news_model/final\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "MODEL_SAVE_PATH = \"./roberta_ag_news_model/final\"\n",
    "\n",
    "trainer.save_model(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÆ Part 8: Inference with the Fine-tuned Model\n",
    "\n",
    "Now let's test our model on some new news headlines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded for inference!\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "inference_model.to(device)\n",
    "inference_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"‚úÖ Model loaded for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_news_topic(text):\n",
    "    \"\"\"\n",
    "    Predicts the topic category for a news article.\n",
    "    \n",
    "    Args:\n",
    "        text: The news article or headline text\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with predicted topic and confidence\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = inference_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        \"topic\": label_names[predicted_class],\n",
    "        \"topic_id\": predicted_class,\n",
    "        \"confidence\": confidence,\n",
    "        \"all_probabilities\": {\n",
    "            label_names[i]: f\"{prob:.2%}\" \n",
    "            for i, prob in enumerate(probabilities[0].cpu().numpy())\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóûÔ∏è News Topic Classification Results\n",
      "======================================================================\n",
      "\n",
      "üì∞ Article 1:\n",
      "   \"UN Security Council meets to discuss Middle East peace negotiations amid rising ...\"\n",
      "   ‚Üí Predicted Topic: üåç World\n",
      "   ‚Üí Confidence: 99.93%\n",
      "\n",
      "üì∞ Article 2:\n",
      "   \"Manchester United defeats Liverpool 3-2 in thrilling Premier League match as Ras...\"\n",
      "   ‚Üí Predicted Topic: üåç World\n",
      "   ‚Üí Confidence: 99.89%\n",
      "\n",
      "üì∞ Article 3:\n",
      "   \"Stock markets rally as Federal Reserve signals potential interest rate cuts in u...\"\n",
      "   ‚Üí Predicted Topic: üíº Business\n",
      "   ‚Üí Confidence: 99.00%\n",
      "\n",
      "üì∞ Article 4:\n",
      "   \"NASA's James Webb telescope discovers water vapor on distant exoplanet, raising ...\"\n",
      "   ‚Üí Predicted Topic: üî¨ Sci/Tech\n",
      "   ‚Üí Confidence: 94.04%\n",
      "\n",
      "üì∞ Article 5:\n",
      "   \"Apple announces record quarterly earnings driven by strong iPhone 15 sales in As...\"\n",
      "   ‚Üí Predicted Topic: üî¨ Sci/Tech\n",
      "   ‚Üí Confidence: 98.13%\n",
      "\n",
      "üì∞ Article 6:\n",
      "   \"World leaders gather in Paris for annual climate summit to address global warmin...\"\n",
      "   ‚Üí Predicted Topic: üåç World\n",
      "   ‚Üí Confidence: 54.27%\n",
      "\n",
      "üì∞ Article 7:\n",
      "   \"OpenAI releases GPT-5 with unprecedented language understanding capabilities.\"\n",
      "   ‚Üí Predicted Topic: üî¨ Sci/Tech\n",
      "   ‚Üí Confidence: 99.89%\n",
      "\n",
      "üì∞ Article 8:\n",
      "   \"LeBron James becomes NBA's all-time leading scorer with spectacular performance.\"\n",
      "   ‚Üí Predicted Topic: üåç World\n",
      "   ‚Üí Confidence: 99.42%\n"
     ]
    }
   ],
   "source": [
    "# Test headlines - one from each category\n",
    "test_headlines = [\n",
    "    # World news\n",
    "    \"UN Security Council meets to discuss Middle East peace negotiations amid rising tensions between neighboring nations.\",\n",
    "    \n",
    "    # Sports news\n",
    "    \"Manchester United defeats Liverpool 3-2 in thrilling Premier League match as Rashford scores winning goal in injury time.\",\n",
    "    \n",
    "    # Business news\n",
    "    \"Stock markets rally as Federal Reserve signals potential interest rate cuts in upcoming quarterly review meeting.\",\n",
    "    \n",
    "    # Science/Tech news\n",
    "    \"NASA's James Webb telescope discovers water vapor on distant exoplanet, raising hopes for potential extraterrestrial life.\",\n",
    "    \n",
    "    # Additional mixed examples\n",
    "    \"Apple announces record quarterly earnings driven by strong iPhone 15 sales in Asian markets.\",\n",
    "    \"World leaders gather in Paris for annual climate summit to address global warming concerns.\",\n",
    "    \"OpenAI releases GPT-5 with unprecedented language understanding capabilities.\",\n",
    "    \"LeBron James becomes NBA's all-time leading scorer with spectacular performance.\"\n",
    "]\n",
    "\n",
    "print(\"üóûÔ∏è News Topic Classification Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, headline in enumerate(test_headlines, 1):\n",
    "    result = predict_news_topic(headline)\n",
    "    print(f\"\\nüì∞ Article {i}:\")\n",
    "    print(f\"   \\\"{headline[:80]}...\\\"\" if len(headline) > 80 else f\"   \\\"{headline}\\\"\")\n",
    "    print(f\"   ‚Üí Predicted Topic: {result['topic']}\")\n",
    "    print(f\"   ‚Üí Confidence: {result['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detailed Prediction Analysis\n",
      "==================================================\n",
      "\n",
      "Text: Tesla stock surges 15% after announcing breakthrough in battery technology for electric vehicles.\n",
      "\n",
      "üìä Predicted Topic: üíº Business\n",
      "\n",
      "üìà Probability Distribution:\n",
      "   üåç World: 0.91% \n",
      "   ‚öΩ Sports: 0.01% \n",
      "   üíº Business: 97.56% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   üî¨ Sci/Tech: 1.52% \n"
     ]
    }
   ],
   "source": [
    "# Detailed prediction with all probabilities\n",
    "sample_text = \"Tesla stock surges 15% after announcing breakthrough in battery technology for electric vehicles.\"\n",
    "\n",
    "result = predict_news_topic(sample_text)\n",
    "\n",
    "print(\"üîç Detailed Prediction Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nText: {sample_text}\")\n",
    "print(f\"\\nüìä Predicted Topic: {result['topic']}\")\n",
    "print(f\"\\nüìà Probability Distribution:\")\n",
    "for topic, prob in result['all_probabilities'].items():\n",
    "    bar_length = int(float(prob.strip('%')) / 5)\n",
    "    bar = \"‚ñà\" * bar_length\n",
    "    print(f\"   {topic}: {prob} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ó Part 9: Using Pipeline for Easy Inference\n",
    "\n",
    "Hugging Face provides a convenient `pipeline` API for even simpler inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline created!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text classification pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=MODEL_SAVE_PATH,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    top_k=None  # Return all classes with probabilities\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pipeline created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Quick Classification with Pipeline\n",
      "==================================================\n",
      "\n",
      "üì∞ Scientists develop new AI algorithm that can predict weather patterns ...\n",
      "   ‚Üí Sci/Tech (98.43%)\n",
      "\n",
      "üì∞ Brazil wins World Cup after penalty shootout against Argentina in hist...\n",
      "   ‚Üí World (99.89%)\n",
      "\n",
      "üì∞ Amazon acquires competitor in $50 billion deal, largest tech merger th...\n",
      "   ‚Üí Sci/Tech (97.99%)\n"
     ]
    }
   ],
   "source": [
    "# Quick classification with pipeline\n",
    "news_articles = [\n",
    "    \"Scientists develop new AI algorithm that can predict weather patterns with 99% accuracy.\",\n",
    "    \"Brazil wins World Cup after penalty shootout against Argentina in historic final.\",\n",
    "    \"Amazon acquires competitor in $50 billion deal, largest tech merger this year.\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Quick Classification with Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for article in news_articles:\n",
    "    results = classifier(article)\n",
    "    top_prediction = max(results[0], key=lambda x: x['score'])\n",
    "    \n",
    "    print(f\"\\nüì∞ {article[:70]}...\")\n",
    "    print(f\"   ‚Üí {top_prediction['label']} ({top_prediction['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì§ Part 10: Pushing to Hugging Face Hub (Optional)\n",
    "\n",
    "Share your fine-tuned model with the world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face Hub\n",
    "# Uncomment and run if you want to push to Hub\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Push to Hub\n",
    "# # Uncomment and modify to push your model\n",
    "\n",
    "# from huggingface_hub import create_repo\n",
    "\n",
    "# # Use the repo_id that trainer already has (the old name)\n",
    "# repo_id = \"your-username/roberta_ag_news_model\"\n",
    "\n",
    "# # Step 1: Create the repository with the trainer's expected name\n",
    "# print(f\"üì¶ Creating repository: {repo_id}\")\n",
    "# create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
    "# print(f\"‚úÖ Repository created: {repo_id}\")\n",
    "\n",
    "# # Step 2: Push using trainer (includes training metrics in model card)\n",
    "# print(f\"\\nüöÄ Pushing model to: {repo_id}\")\n",
    "# trainer.push_to_hub()\n",
    "\n",
    "# # Step 3: Push tokenizer\n",
    "# tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "# print(f\"\\n‚úÖ Model successfully pushed with training info!\")\n",
    "# print(f\"üîó View your model at: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **Fine-tuned RoBERTa-base** on the complete AG News dataset (120,000 training samples)\n",
    "2. **Trained for 3 epochs** with proper evaluation metrics (accuracy and F1 score)\n",
    "3. **Built inference pipeline** for easy prediction on new articles\n",
    "\n",
    "### Model Performance:\n",
    "\n",
    "RoBERTa-base typically achieves **~94-95% accuracy** on AG News after 3 epochs of training!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different learning rates (1e-5 to 5e-5)\n",
    "- Experiment with more epochs (4-5)\n",
    "- Compare with other models (BERT, ALBERT, XLNet)\n",
    "- Add more sophisticated evaluation (confusion matrix, per-class metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö References & Resources\n",
    "\n",
    "- [RoBERTa Paper](https://arxiv.org/abs/1907.11692) - \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"\n",
    "- [AG News Dataset](https://huggingface.co/datasets/fancyzhx/ag_news) - Dataset on Hugging Face\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)\n",
    "- [Fine-tuning Guide](https://huggingface.co/docs/transformers/training)\n",
    "- [FacebookAI/roberta-base](https://huggingface.co/FacebookAI/roberta-base) - Model Card\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
